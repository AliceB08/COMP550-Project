{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP550 - Final Project\n",
    "---\n",
    "\n",
    "Links:\n",
    "- https://www.kaggle.com/ficklemaverick/lyrics-generator\n",
    "- https://www.kaggle.com/danofer/music-lyrics-clean-export\n",
    "\n",
    "## Table of content\n",
    "[1. Imports](#imports)  \n",
    "[2. Import & Cleaning data and Exploratory Data Analysis](#imports-clean)  \n",
    "[3. Preprocessing steps](#preprocessing)  \n",
    "[4. Naïve majority model](#naive-model)   \n",
    "[5. Logistic Regression](#log-reg)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports  <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from langdetect import detect\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# nltk imports\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import & Cleaning data and Exploratory Data Analysis   <a class=\"anchor\" id=\"imports-clean\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Detecting the language of all the songs is very long (15 minutes). To avoid this step, you can use the `cleaned_data.csv` instead of running all the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CLEANED_DATA = True\n",
    "cleaned_data_path = \"data/cleaned_data.csv\"\n",
    "data_path = \"data/lyrics.csv\"\n",
    "data_raw = pd.read_csv(data_path)\n",
    "print(len(data_raw), \"songs in the dataset\")\n",
    "print(data_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has the following columns:\n",
    "- **index** (int): index of the song in the dataset\n",
    "- **song** (string): name of the song\n",
    "- **year** (float) -> (int): release year\n",
    "- **artist** (string): artist of the song\n",
    "- **genre** (string): the genre, this is the label we want to predict\n",
    "- **lyrics** (string): the lyrics of the song. This is the data we will use to predict the genre. We need to preprocess this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we remove the null elements, we are left with **265701** songs. We convert the year from float to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_raw[pd.notnull(data_raw)]\n",
    "data_all = data_all.dropna(how='any',axis=0)\n",
    "data_all['year'] = pd.to_numeric(data_all['year'], downcast='integer')\n",
    "data_all['index'] = pd.to_numeric(data_all['index'], downcast='integer')\n",
    "data_all = data_all.reset_index(drop=True)\n",
    "data_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep only English songs, using the `langdetect` library. There are **237,363 English songs** in the previous 265,701 songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_CLEANED_DATA:\n",
    "    en_songs = []\n",
    "    for song in data_all['lyrics']:\n",
    "        try:\n",
    "            lang = detect(song)\n",
    "            if lang == 'en':\n",
    "                en_songs.append(True)\n",
    "            else:\n",
    "                en_songs.append(False)\n",
    "        except:\n",
    "            en_songs.append(False)\n",
    "    data_en = data_all[en_songs]\n",
    "    data_en.reset_index(drop=True)\n",
    "else:\n",
    "    data_en = pd.read_csv(cleaned_data_path)\n",
    "    \n",
    "data_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en['genre'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove songs where the labels are \"Other\" or \"Not Available\". This reduces the number of songs from 237363 to **215,825 songs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_en['genre'].tolist()\n",
    "keep_song = [genre not in ['Not Available', 'Other'] for genre in data_en['genre'].tolist()]\n",
    "data_en = data_en[keep_song]\n",
    "data_en = data_en.reset_index(drop=True)\n",
    "data_en['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the the number of songs in each genre category. **46,5% of the songs are Rock songs**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a smaller dataset with 10,000 songs to do quick testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENRE_TO_INT = {'Pop':0, 'Hip-Hop':1, 'Rock':2, 'Metal':3, 'Country':4, 'Jazz':5, 'Electronic':6, 'Folk':7, 'R&B':8, 'Indie':9}\n",
    "lyrics = data_en['lyrics'].tolist()\n",
    "labels = np.array([GENRE_TO_INT[genre] for genre in data_en['genre'].tolist()])\n",
    "lyrics_train, lyrics_test, labels_train, labels_test = train_test_split(lyrics, labels, test_size=0.1, shuffle=True, random_state=43, stratify=labels)\n",
    "lyrics_train, lyrics_valid, labels_train, labels_valid = train_test_split(lyrics_train, labels_train, test_size=0.1, shuffle=True, random_state=43, stratify=labels_train)\n",
    "\n",
    "# Smaller dataset for wuick training\n",
    "lyrics_other, lyrics_light, labels_other, labels_light = train_test_split(lyrics_train, labels_train, test_size=10000, shuffle=True, random_state=43, stratify=labels_train)\n",
    "\n",
    "lyrics_other, lyrics_medium, labels_other, labels_medium = train_test_split(lyrics_other, labels_other, test_size=50000, shuffle=True, random_state=43, stratify=labels_other)\n",
    "print(\"Light training set length:\", len(lyrics_light))\n",
    "print(\"Training set length:\", len(lyrics_train))\n",
    "print(\"Validation set length:\", len(lyrics_valid))\n",
    "print(\"Test set length:\", len(lyrics_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_light[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing steps <a class=\"anchor\" id=\"preprocessing\"></a>\n",
    "The lyrics need to be cleaned before we can use them.\n",
    "- remove \\n line breaks\n",
    "- remove punctuation\n",
    "- lowercase the lyrics\n",
    "- remove verse and chorus indications that are under the form [verse x]\n",
    "- remove tokens that have a null length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lyrics_light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace line breaks, removes punctuation, set everything to lowercase\n",
    "# removes word if length <= 2, [verse X] or [chorus y] indication\n",
    "# remove stopwords\n",
    "def my_preprocessor(song):\n",
    "    song = song.replace('\\n', ' ')\n",
    "    song = song.translate(str.maketrans('', '', string.punctuation))\n",
    "    song = song.lower()\n",
    "    song_token = song.split(' ')\n",
    "    song_token = [w for w in song_token if (len(w) >= 3 and w[0] != '[' and w[-1] != ']')]\n",
    "    song_token = [w for w in song_token if not any(c.isdigit() for c in w)]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    song_token = [w for w in song_token if (w not in stop_words and u'\\x9d' not in w)]\n",
    "    song = ' '.join(song_token)\n",
    "    return song\n",
    "\n",
    "# tokenize the song\n",
    "def my_tokenizer(song): \n",
    "    tokens = song.split(' ')\n",
    "    return tokens\n",
    "\n",
    "# tokenize the song and stems its tokens\n",
    "def my_tokenizer_stem(song): \n",
    "    tokens = song.split(' ') \n",
    "    stemmer = PorterStemmer() \n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "# tokenize the song and lemmas its tokens\n",
    "def my_tokenizer_lemma(song):\n",
    "    song_with_pos = pos_tag(song.split(' '))\n",
    "    POS_correspondance = {'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV, 'J': wordnet.ADJ}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_song = [lemmatizer.lemmatize(w[0], POS_correspondance.get(w[1][0], wordnet.NOUN)) for w in song_with_pos]\n",
    "    return lemmatized_song\n",
    "\n",
    "print(my_tokenizer(my_preprocessor(data[0]))[:10])\n",
    "print(my_tokenizer_stem(my_preprocessor(data[0]))[:10])\n",
    "print(my_tokenizer_lemma(my_preprocessor(data[0]))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wm2df(wm, feat_names):\n",
    "    \n",
    "    # create an index for each row\n",
    "    doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
    "    df = pd.DataFrame(data=wm.toarray(), index=doc_names,\n",
    "                      columns=feat_names)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(preprocessor=my_preprocessor, tokenizer=my_tokenizer_stem,\n",
    "#                              max_features=5000, ngram_range=(1,1))\n",
    "# X = vectorizer.fit_transform(data)\n",
    "# tokens = vectorizer.get_feature_names()\n",
    "# print(tokens)\n",
    "# wm2df(X, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Naïve Majority Model  <a class=\"anchor\" id=\"naive-model\"></a>\n",
    "In this naïve majority model, we guess that all the songs have the genre 'Rock', which is the genre that has the majority of songs. This is a first baseline model, that we can use to compare the results of logistic regression, naive bayes, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_light, [2]*len(labels_light), target_names=list(GENRE_TO_INT.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision for our baseline model is **47%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logistic Regression  <a class=\"anchor\" id=\"log-reg\"></a>\n",
    "We test different forms of the vectorised data: stemmed, lemmatized and no token transformation. The step to vectorize the data is quite long so we decide to test different hyperparameters of a model AFTER the vectorization is performed.\n",
    "\n",
    "Solver: liblinear. Although it doesn't support multinomial, it's the quickest solver (by far) and is the only one that converges after 1000 iterations.\n",
    "\n",
    "Best accuracy with the following model : Best with stemming: **55,9%** {C: 0.1, max_df: 0.7, max_features: 150000, ngram_range: bigram}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the different solvers\n",
    "newton-cg: 100sec (not converged), acc=95/51%  \n",
    "sag: 100 sec (not converged), acc=94/51%  \n",
    "saga: 116 sec, acc=88/52%  \n",
    "lbfgs: sec, acc=  \n",
    "liblinear: 57 sec, acc=88/51%\n",
    "\n",
    "The quickest solver is **liblinear**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_accuracy(lyrics, labels, vectorizer, classifier, show_accuracy=False):\n",
    "    start = time.time()\n",
    "    # Using k-folds\n",
    "    NB_FLODS = 3\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=NB_FLODS, shuffle=True)\n",
    "    lyrics_vec = vectorizer.fit_transform(lyrics)\n",
    "\n",
    "    mean_train_accuracy = 0\n",
    "    mean_valid_accuracy = 0\n",
    "    \n",
    "    for train_index, valid_index in skf.split(lyrics_vec, labels):\n",
    "        _X_train, _X_valid = lyrics_vec[train_index], lyrics_vec[valid_index]\n",
    "        _y_train, _y_valid = labels[train_index], labels[valid_index]\n",
    "        classifier.fit(_X_train, _y_train)\n",
    "        mean_train_accuracy += accuracy_score(_y_train, classifier.predict(_X_train))\n",
    "        mean_valid_accuracy += accuracy_score(_y_valid, classifier.predict(_X_valid))\n",
    "\n",
    "    mean_train_accuracy /= NB_FLODS\n",
    "    mean_valid_accuracy /= NB_FLODS\n",
    "    \n",
    "    end = time.time()\n",
    "    print('Done in ', end - start)\n",
    "\n",
    "    if show_accuracy:\n",
    "        print(\"Training accuracy:\", mean_train_accuracy)\n",
    "        print(\"Validation accuracy:\", mean_valid_accuracy)\n",
    "\n",
    "    return mean_train_accuracy, mean_valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "classifier = LogisticRegression(multi_class='auto', solver='liblinear', max_iter=1000)\n",
    "# mean_train_accuracy, mean_valid_accuracy = get_model_accuracy(lyrics_light, labels_light, vectorizer, classifier)\n",
    "# print(mean_train_accuracy, mean_valid_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "With a small dataset (10,000 songs) we grid search on the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_light_preprocessed = [my_preprocessor(song) for song in lyrics_light]\n",
    "lyrics_light_stemmed = [my_tokenizer_stem(song) for song in lyrics_light_preprocessed]\n",
    "lyrics_light_lemmad = [my_tokenizer_lemma(song) for song in lyrics_light_preprocessed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i/20 for i in range(1, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline combining a text feature extractor with a simple classifier\n",
    "GRID_SEARCH_ON = False\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', LogisticRegression(multi_class='auto', solver='liblinear', penalty='l1', max_iter=100)),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95],\n",
    "    'vect__max_features': [20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000, 150000, 160000, 170000, 180000, 190000],\n",
    "    'vect__ngram_range': [(1, 2)],\n",
    "    'clf__C': [0.09, 0.1, 0.11],\n",
    "}\n",
    "\n",
    "# find the best parameters for both the feature extraction and the classifier\n",
    "if GRID_SEARCH_ON:\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1, verbose=1)\n",
    "    start = time.time()\n",
    "    grid_search.fit(lyrics_light_stemmed, labels_light)\n",
    "    end = time.time()\n",
    "    print(\"done in %0.3fs\" % (end - start))\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results grid search \n",
    "The different hyperparameters and ranges why are testing:\n",
    "- **tokenizer**: [my_tokenizer, my_tokenizer_stem, my_tokenizer_lemma]. The best results are with no modification (surprisingly). Lemmatization 56,1%; stemming: 56,3%; no token modification: 55,3%%.\n",
    "- **max_df**: range(0.3, 1). 1 always gives the worst results but values between 0.6 and 0.9 have similar results. There is no best value, we have to combine max_df with other parameters to have the best combination.\n",
    "- **max_features**: range(10000, 200000). Best results with a number of features around 100000 AND bigram models. For a unigram model best results are with max_features around 30000.\n",
    "- **ngram_range**: unigrams, bigrams. Bigram models have the best results (slightly).\n",
    "- **C**: range(0.01, 1). The regularization strength is the most important parameter to finetune. A value around 0.1 increases the accuracy up to 10% compared to a bad choice of strength.\n",
    "\n",
    "There are a lot of different possible combinations. Here is the methodology for grid search.\n",
    "1. Try out 3 different values for each hyperparameter (min, max, middle) and see which parameters modify the most the accuracy. For example the tokenizer doesn't change the accuracy that much, but the regularization strength affects a lot the accuracy.\n",
    "2. For each hyperparameter that doesn't have a big impacy, chose the value that gives the highest accuracy. If there is no trend (for example the hyperparameter sometimes give better results with a certain value and other times a worst result, take the value that has the smallest computation time).\n",
    "3. The value of regularization strength is the most important hyperparameter to determine. A value around 0.1 is a good choice.\n",
    "4. Little by little, trim the ranges of the hyperparameter choices, taking each time the one that affects the most the accuracy.\n",
    "\n",
    "The best model gives **56,4% accuracy** has the following parameters: **no token modification, C=0.12, max_df=0.6, max_features=120000, bigrams**.\n",
    "\n",
    "No modification: lots of lyrics are modified, espacially in rap & pop. Maybe that's why stemming or lemmatizing results are slightly worst.\n",
    "\n",
    "Best with no tokenization modification: **55,36%** {C: 0,07, max_df: 0,7, max_features: 100000, ngram_range: bigram}  \n",
    "Best with stemming: **55,9%** {C: 0.1, max_df: 0.7, max_features: 150000, ngram_range: bigram}  \n",
    "Best with lemmatization: **55,6%** {C: 0.14, max_df: 0.7, max_features: 150000, ngram_range: bigram}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_df = pd.DataFrame.from_dict(grid_search.cv_results_)\n",
    "# grid_search_df.to_csv(\"data/result_reglog_liblinear_stem_4.csv\", sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=my_tokenizer, max_df=0.6, max_features=120000, ngram_range=(1, 2))\n",
    "classifier = LogisticRegression(multi_class='auto', solver='liblinear', max_iter=1000, C=0.12)\n",
    "lyrics_train_vec = vectorizer.fit_transform(lyrics_train)\n",
    "lyrics_valid_vec = vectorizer.fit_transform(lyrics_valid)\n",
    "classifier.fit(lyrics_train_vec, labels_train)\n",
    "accuracy_score(labels_valid, classifier.predict(lyrics_valid_vec))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
