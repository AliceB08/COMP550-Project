{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP550 - Final Project\n",
    "---\n",
    "\n",
    "Links:\n",
    "- https://www.kaggle.com/ficklemaverick/lyrics-generator\n",
    "- https://www.kaggle.com/danofer/music-lyrics-clean-export\n",
    "\n",
    "## Table of content\n",
    "[1. Imports](#imports)  \n",
    "[2. Import & Cleaning data and Exploratory Data Analysis](#imports-clean)  \n",
    "[3. Preprocessing steps](#preprocessing)  \n",
    "[4. Naïve majority model](#naive-model)   \n",
    "[5. Logistic Regression](#log-reg)  \n",
    "[6. Naïve Bayes](#naive-bayes)  \n",
    "[7. Support Vector Machine](#SVM)  \n",
    "[8. Sequencial model - LSTM](#LSTM)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports  <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_GOOGLE_COLAB = False\n",
    "root_path = 'data/'\n",
    "if IN_GOOGLE_COLAB:\n",
    "    !pip install langdetect\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_path = 'drive/My Drive/COMP550-Project/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from langdetect import detect\n",
    "from scipy.stats import uniform\n",
    "import warnings\n",
    "import multiprocessing\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "# nltk imports\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "# gensim imports\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "if not IN_GOOGLE_COLAB:\n",
    "    # pytorch imports\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    from torch.utils import data\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    from torch.autograd import Variable\n",
    "    torch.manual_seed(1)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import & Cleaning data and Exploratory Data Analysis   <a class=\"anchor\" id=\"imports-clean\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Detecting the language of all the songs is very long (15 minutes). To avoid this step we import directly the preprocessed english that is split into 3 sets: training, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CLEANED_DATA = True\n",
    "cleaned_data_path = root_path + \"cleaned_data.csv\"\n",
    "data_path = root_path + \"lyrics.csv\"\n",
    "data_raw = pd.read_csv(data_path)\n",
    "print(len(data_raw), \"songs in the dataset\")\n",
    "print(data_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has the following columns:\n",
    "- **index** (int): index of the song in the dataset\n",
    "- **song** (string): name of the song\n",
    "- **year** (float) -> (int): release year\n",
    "- **artist** (string): artist of the song\n",
    "- **genre** (string): the genre, this is the label we want to predict\n",
    "- **lyrics** (string): the lyrics of the song. This is the data we will use to predict the genre. We need to preprocess this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we remove the null elements, we are left with **265701** songs. We convert the year from float to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_raw[pd.notnull(data_raw)]\n",
    "data_all = data_all.dropna(how='any',axis=0)\n",
    "data_all['year'] = pd.to_numeric(data_all['year'], downcast='integer')\n",
    "data_all['index'] = pd.to_numeric(data_all['index'], downcast='integer')\n",
    "data_all = data_all.reset_index(drop=True)\n",
    "data_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep only English songs, using the `langdetect` library. There are **237,363 English songs** in the previous 265,701 songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_CLEANED_DATA:\n",
    "    en_songs = []\n",
    "    for song in data_all['lyrics']:\n",
    "        try:\n",
    "            lang = detect(song)\n",
    "            if lang == 'en':\n",
    "                en_songs.append(True)\n",
    "            else:\n",
    "                en_songs.append(False)\n",
    "        except:\n",
    "            en_songs.append(False)\n",
    "    data_en = data_all[en_songs]\n",
    "    data_en.reset_index(drop=True)\n",
    "else:\n",
    "    data_en = pd.read_csv(cleaned_data_path)\n",
    "    \n",
    "data_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en['genre'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove songs where the labels are \"Other\" or \"Not Available\". This reduces the number of songs from 237363 to **215,825 songs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_en['genre'].tolist()\n",
    "keep_song = [genre not in ['Not Available', 'Other'] for genre in data_en['genre'].tolist()]\n",
    "data_en = data_en[keep_song]\n",
    "data_en = data_en.reset_index(drop=True)\n",
    "data_en['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the the number of songs in each genre category. **46,5% of the songs are Rock songs**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing steps <a class=\"anchor\" id=\"preprocessing\"></a>\n",
    "The lyrics need to be cleaned before we can use them.\n",
    "- remove \\n line breaks\n",
    "- remove punctuation\n",
    "- lowercase the lyrics\n",
    "- remove verse and chorus indications that are under the form [verse x]\n",
    "- remove tokens that have a null length\n",
    "\n",
    "Because this step is long, we saved the preprocesed dataset in the csv `final_lyrics_dataframe.csv`. There are 5 columns: genre (int), lyrics (whole lyrics sequence, preprcesed without removing stopwords, this will be used in the LSTM), lyrics_no_stop_words (preprocessed lyrics with no stopwords), lyrics_stemmd (stemmed lyrics), lyrics_lemamd (lemmatized lyrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_FINAL_LYRICS_DATAFRAME = True\n",
    "if LOAD_FINAL_LYRICS_DATAFRAME:\n",
    "    df_balanced_final = pd.read_csv(root_path + \"final_lyrics_dataframe.csv\")\n",
    "\n",
    "# replace line breaks, removes punctuation, set everything to lowercase\n",
    "# removes word if length <= 2, [verse X] or [chorus y] indication\n",
    "# remove stopwords\n",
    "GENRE_TO_INT = {'Pop':0, 'Hip-Hop':1, 'Rock':2, 'Metal':3, 'Country':4, 'Jazz':5, 'Electronic':6, 'Folk':7, 'R&B':8, 'Indie':9}\n",
    "INT_TO_GENRE = {0:'Pop', 1:'Hip-Hop', 2:'Rock', 3:'Metal', 4:'Country', 5:'Jazz', 6:'Electronic', 7:'Folk', 8:'R&B', 9:'Indie'}\n",
    "\n",
    "if not IN_GOOGLE_COLAB:\n",
    "    def my_preprocessor(song, remove_stopwords=True):\n",
    "        song = song.replace('\\n', ' ')\n",
    "        song = song.translate(str.maketrans('', '', string.punctuation))\n",
    "        song = song.lower()\n",
    "        song_token = song.split(' ')\n",
    "        song_token = [w for w in song_token if (len(w) >= 3 and w[0] != '[' and w[-1] != ']')]\n",
    "        song_token = [w for w in song_token if not any(c.isdigit() for c in w)]\n",
    "        if remove_stopwords:\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            song_token = [w for w in song_token if (w not in stop_words)]\n",
    "        song = ' '.join(song_token)\n",
    "        song = re.sub(r'[^a-z ]+', '', song)\n",
    "        return song\n",
    "\n",
    "    # tokenize the song\n",
    "    def my_tokenizer(song): \n",
    "        tokens = song.split(' ')\n",
    "        return tokens\n",
    "\n",
    "    # tokenize the song and stems its tokens\n",
    "    def my_tokenizer_stem(song): \n",
    "        tokens = song.split(' ') \n",
    "        stemmer = PorterStemmer() \n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        return stemmed_tokens\n",
    "\n",
    "    # tokenize the song and lemmas its tokens\n",
    "    def my_tokenizer_lemma(song):\n",
    "        song=[w for w in song.split(' ') if len(w)>0]\n",
    "        song_with_pos = pos_tag(song)\n",
    "        POS_correspondance = {'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV, 'J': wordnet.ADJ}\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_song = [lemmatizer.lemmatize(w[0], POS_correspondance.get(w[1][0], wordnet.NOUN)) for w in song_with_pos]\n",
    "        return lemmatized_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_FINAL_LYRICS_DATAFRAME:\n",
    "    # preprocess the songs\n",
    "    LSTM_lyrics = [my_preprocessor(song, remove_stopwords=False) for song in data_en['lyrics'].to_numpy()]\n",
    "    no_stopwords_lyrics = [my_preprocessor(song, remove_stopwords=True) for song in data_en['lyrics'].to_numpy()]\n",
    "    genres_int = [GENRE_TO_INT[genre] for genre in data_en['genre']]\n",
    "\n",
    "    # only keep songs with more than 20 words\n",
    "    keep_song_LSTM = []\n",
    "    keep_song_no_stopwords = []\n",
    "    keep_genre = []\n",
    "    for i, song in enumerate(LSTM_lyrics):\n",
    "        if len(song) > 20:\n",
    "            keep_song_LSTM.append(song)\n",
    "            keep_song_no_stopwords.append(no_stopwords_lyrics[i])        \n",
    "            keep_genre.append(genres_int[i])\n",
    "\n",
    "    df_final = pd.DataFrame.from_dict({'lyrics': keep_song_LSTM, 'no_stop_words_lyrics': keep_song_no_stopwords, 'genre': keep_genre})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance dataset\n",
    "Keep only 1778 songs per genre, totalling 17,780 songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_FINAL_LYRICS_DATAFRAME:\n",
    "    # Keep only 1778 songs per genre, totalling 17,780 songs\n",
    "\n",
    "    def get_balanced(df):\n",
    "        genre_dict={i:0 for i in range(10)}\n",
    "        df_mix = df.sample(frac=1, random_state = 43)\n",
    "        df_mix=df_mix.reset_index(drop=True)\n",
    "        song_to_keep = []\n",
    "        for i, lyrics in enumerate(df_mix['lyrics']):\n",
    "            genre=df_mix['genre'][i]\n",
    "            if genre_dict[genre] < 1778:\n",
    "                genre_dict[genre] += 1\n",
    "                song_to_keep.append(True)\n",
    "            else:\n",
    "                song_to_keep.append(False)\n",
    "        df_balanced = df_mix[song_to_keep].reset_index(drop=True)\n",
    "        df_balanced = df_balanced.sample(frac=1, random_state = 43).reset_index(drop=True)\n",
    "        return df_balanced\n",
    "\n",
    "    df_balanced_final = get_balanced(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem and lemmatize songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_FINAL_LYRICS_DATAFRAME:\n",
    "    df_balanced_final['lyrics_stemmed'] = [' '.join(my_tokenizer_stem(song)) for song in df_balanced_final['no_stop_words_lyrics']]\n",
    "    df_balanced_final['lyrics_lemmad'] = [' '.join(my_tokenizer_lemma(song)) for song in df_balanced_final['no_stop_words_lyrics']]\n",
    "    df_balanced_final.to_csv(root_path+\"final_lyrics_dataframe.csv\", index=False)\n",
    "df_balanced_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We split the data in training, validation and test sets\n",
    "The english songs are smplit into 3 sets : 90% for training, 5% for validation and 5% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_train, lyrics_valid, labels_train, labels_valid = train_test_split(df_balanced_final, df_balanced_final['genre'], test_size=0.1, shuffle=True, random_state=43, stratify=df_balanced_final['genre'])\n",
    "lyrics_test, lyrics_valid, labels_test, labels_valid = train_test_split(lyrics_valid, labels_valid, test_size=0.5, shuffle=True, random_state=43, stratify=labels_valid)\n",
    "\n",
    "train_lyrics_preprocessed = lyrics_train['no_stop_words_lyrics'].to_numpy()\n",
    "train_lyrics = lyrics_train['lyrics'].to_numpy()\n",
    "train_lyrics_stemmed = lyrics_train['lyrics_stemmed'].to_numpy()\n",
    "train_lyrics_lemmad = lyrics_train['lyrics_lemmad'].to_numpy()\n",
    "train_labels = labels_train.to_numpy()\n",
    "\n",
    "valid_lyrics_preprocessed = lyrics_valid['no_stop_words_lyrics'].to_numpy()\n",
    "valid_lyrics = lyrics_valid['lyrics'].to_numpy()\n",
    "valid_lyrics_stemmed = lyrics_valid['lyrics_stemmed'].to_numpy()\n",
    "valid_lyrics_lemmad = lyrics_valid['lyrics_lemmad'].to_numpy()\n",
    "valid_labels = labels_valid.to_numpy()\n",
    "\n",
    "test_lyrics_preprocessed = lyrics_test['no_stop_words_lyrics'].to_numpy()\n",
    "test_lyrics = lyrics_test['lyrics'].to_numpy()\n",
    "test_lyrics_stemmed = lyrics_test['lyrics_stemmed'].to_numpy()\n",
    "test_lyrics_lemmad = lyrics_test['lyrics_lemmad'].to_numpy()\n",
    "test_labels = labels_test.to_numpy()\n",
    "\n",
    "print(\"Training set length:\", len(lyrics_train))\n",
    "print(\"Validation set length:\", len(lyrics_valid))\n",
    "print(\"Test set length:\", len(lyrics_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Naïve Majority Model  <a class=\"anchor\" id=\"naive-model\"></a>\n",
    "In this naïve majority model, we guess that all the songs have the genre 'Rock', which is the genre that has the majority of songs. This is a first baseline model, that we can use to compare the results of logistic regression, naive bayes, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_train, [2]*len(lyrics_train), target_names=list(GENRE_TO_INT.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision for our baseline model is **10%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logistic Regression  <a class=\"anchor\" id=\"log-reg\"></a>\n",
    "We test different forms of the vectorised data: stemmed, lemmatized and no token transformation. The step to vectorize the data is quite long so we decide to test different hyperparameters of a model AFTER the vectorization is performed.\n",
    "\n",
    "##### With no hyperparameter tuning:\n",
    "Accuracy on training set: 0.9474%  \n",
    "Accuracy on test set: 0.5452%  \n",
    "\n",
    "#### Best model with k-fold cross validation\n",
    "Best accuracy with the following model : **58,6%** no stemming or lemmatization, TD-IDF vectorization, regularization strength C: 0.1, max_df: 0.7, max_features: 150000, ngram_range: bigram}.\n",
    "On development set: **59,52%**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "With a small dataset (10,000 songs) we grid search on the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline combining a text feature extractor with a simple classifier\n",
    "GRID_SEARCH_ON = False\n",
    "pipeline = Pipeline([\n",
    "    # ('vect', CountVectorizer()),\n",
    "    ('vect', TfidfVectorizer()),    \n",
    "    ('clf', LogisticRegression(multi_class='auto', solver='lbfgs', penalty='l2', max_iter=100)),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': [0.8],\n",
    "    'vect__max_features': [210000],\n",
    "    'vect__ngram_range': [(1,2)],\n",
    "    'vect__norm': ['l2'],\n",
    "    'clf__C': [2.5, 2.6, 2.7, 2.8, 2.9],\n",
    "}\n",
    "\n",
    "# find the best parameters for both the feature extraction and the classifier\n",
    "if GRID_SEARCH_ON:\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "    start = time.time()\n",
    "    grid_search.fit(train_lyrics_lemmad, train_labels)\n",
    "    end = time.time()\n",
    "    print(\"done in %0.3fs\" % (end - start))\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results grid search \n",
    "The different hyperparameters and ranges why are testing:\n",
    "- **tokenizer**: [my_tokenizer, my_tokenizer_stem, my_tokenizer_lemma].\n",
    "- **max_df**: range(0.3, 1)\n",
    "- **max_features**: range(10000, 200000)\n",
    "- **ngram_range**: unigrams, bigrams. Unigram and bigram models tend to have the best results.\n",
    "- **C**: range(0.01, 3). The regularization strength is the most important parameter to finetune. A value around 0.1 increases the accuracy up to 10% compared to a bad choice of strength. When TF-IDF is on, the strength needs to be around 2.\n",
    "- **TFIDF**: on or off (depends on which vectorizer we use). When turned on, the accuracy is higher.\n",
    "- **norm**: when TFIDF=on defines the unit norm of each row.\n",
    "\n",
    "There are a lot of different possible combinations. Here is the methodology for grid search.\n",
    "0. preprocessing = none\n",
    "1. Try out 3 different values for each hyperparameter (min, max, middle) and see which parameters modify the most the accuracy. For example the tokenizer doesn't change the accuracy that much, but the regularization strength affects a lot the accuracy.\n",
    "2. For each hyperparameter that doesn't have a big impacy, chose the value that gives the highest accuracy. If there is no trend (for example the hyperparameter sometimes give better results with a certain value and other times a worst result, take the value that has the smallest computation time).\n",
    "3. The value of regularization strength is the most important hyperparameter to determine. A value around 0.1 is a good choice.\n",
    "4. Little by little, trim the ranges of the hyperparameter choices, taking each time the one that affects the most the accuracy.\n",
    "5. Repeat from 0 for preprocessing = stemming, lemmatization\n",
    "6. Repeat from 0 for TFIDF = on\n",
    "\n",
    "**TFIDF=off**  \n",
    "Best with no tokenization modification: **55,7%** {C: 0,07, max_df: 0,7, max_features: 100000, ngram_range: bigram}  \n",
    "Best with stemming: **55,9%** {C: 0.1, max_df: 0.7, max_features: 150000, ngram_range: bigram}  \n",
    "Best with lemmatization: **55,5%** {C: 0.14, max_df: 0.7, max_features: 150000, ngram_range: bigram}\n",
    "\n",
    "**TFIDF=on**  \n",
    "Best with no tokenization modification:  **58,6%** {C: 2.6, max_df: 0.5, max_features: 210000, ngram_range: bigram, norm='l2'}  \n",
    "Best with stemming: **58,2%** {C: 2.2, max_df: 0.5, max_features: 25000, ngram_range: bigram, norm='l2'}  \n",
    "Best with lemmatization:  **58,5** {C: 2.8, max_df: 0.8, max_features: 210000, ngram_range: bigram, norm='l2'}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_df = pd.DataFrame.from_dict(grid_search.cv_results_)\n",
    "# grid_search_df.to_csv(root_path+\"result_reglog_tfidf_preprocessed_6.csv\", sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We compute the accuracy of the best model on the validation set\n",
    "\n",
    "With balanced dataset  \n",
    "Accuracy on training set: 97.28%  \n",
    "Accuracy on validation set: 40.71%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train_lyrics_preprocessed, train_lyrics_stemmed, train_lyrics_lemmad, train_labels\n",
    "# vectorizer = TfidfVectorizer(max_df=0.8, max_features=210000, ngram_range=(1, 2), norm='l2')\n",
    "# classifier = LogisticRegression(multi_class='auto', solver='lbfgs', penalty='l2', C=2.8, max_iter=1000)\n",
    "# lyrics_train_vec = vectorizer.fit_transform(train_lyrics_preprocessed)\n",
    "# lyrics_valid_vec = vectorizer.transform(test_lyrics_preprocessed)\n",
    "# classifier.fit(lyrics_train_vec, train_labels)\n",
    "# print(\"Accuracy on training set:\", accuracy_score(train_labels, classifier.predict(lyrics_train_vec)))\n",
    "# print(\"Accuracy on test set:\", accuracy_score(test_labels, classifier.predict(lyrics_valid_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Naïve Bayes Model  <a class=\"anchor\" id=\"naive-bayes\"></a>\n",
    "We test different forms of the vectorised data: stemmed, lemmatized and no token transformation. The step to vectorize the data is quite long so we decide to test different hyperparameters of a model AFTER the vectorization is performed.\n",
    "\n",
    "#### With no hyperparameter tuning:\n",
    "Accuracy on training set: 0.6843%  \n",
    "Accuracy on test set: 0.5856%  \n",
    "\n",
    "#### Best model with k-fold cross validation\n",
    "Best accuracy with TFIDF and stemming on training set: **63,91%**  \n",
    "On validation set: **59,76%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline combining a text feature extractor with a simple classifier\n",
    "GRID_SEARCH_ON = False\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    #('vect', TfidfVectorizer()),    \n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = { \n",
    "    'vect__max_df': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'vect__max_features': [100000,150000,200000],\n",
    "    'vect__ngram_range': [(1,1),(1,2)],\n",
    "    #'vect__norm': ['l2'], #not a parameter for CountVectorizer()\n",
    "    'clf__fit_prior': [True],\n",
    "    'clf__alpha': [1],\n",
    "}\n",
    "\n",
    "# find the best parameters for both the feature extraction and the classifier\n",
    "if GRID_SEARCH_ON:\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "    start = time.time()\n",
    "    grid_search.fit(train_lyrics_lemmad, train_labels)\n",
    "    end = time.time()\n",
    "    print(\"done in %0.3fs\" % (end - start))\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results grid search \n",
    "The different hyperparameters and ranges why are testing:\n",
    "- **tokenizer**: [my_tokenizer, my_tokenizer_stem, my_tokenizer_lemma]. \n",
    "- **max_df**: range(0.3, 1). This value is pretty much 0.2 across the board for all the following experiments.\n",
    "- **max_features**: range(10000, 200000). For TFIDF = off, this value is in the upper end of the range. For TFIDF = on, this value is typically much lower (see results)\n",
    "- **ngram_range**: unigrams, unigrams and bigrams, bigrams. (1,2) is the best performing parameter in the following tests.\n",
    "- **alpha**: float - controls smoothing; 0 is no smoothing, 1 is Laplace smoothing. \n",
    "- **fit_prior**: bool - whether to learn class priors. All test run best with TRUE.\n",
    "- **TFIDF**: on or off (depends on which vectorizer we use).\n",
    "- **norm**: when TFIDF=on defines the unit norm of each row.\n",
    "\n",
    "There are a lot of different possible combinations. Here is the methodology for grid search.\n",
    "0. preprocessing = none\n",
    "1. Try out 3 different values for each hyperparameter (min, max, middle) and see which parameters modify the most the accuracy. For example the tokenizer doesn't change the accuracy that much, but the regularization strength affects a lot the accuracy.\n",
    "2. For each hyperparameter that doesn't have a big impacy, chose the value that gives the highest accuracy. If there is no trend (for example the hyperparameter sometimes give better results with a certain value and other times a worst result, take the value that has the smallest computation time).\n",
    "3. The value of regularization strength is the most important hyperparameter to determine. A value around 0.1 is a good choice.\n",
    "4. Little by little, trim the ranges of the hyperparameter choices, taking each time the one that affects the most the accuracy.\n",
    "5. Repeat from 0 for preprocessing = stemming, lemmatization\n",
    "6. Repeat from 0 for TFIDF = on\n",
    "\n",
    "Initial parameters run as: parameters = { \n",
    "    'vect__max_df': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'vect__max_features': [10000,100000,200000],\n",
    "    'vect__ngram_range': [(1,1),(1,2),(2,2)],\n",
    "    'vect__norm': ['l2','l1'], #not a parameter for CountVectorizer()\n",
    "    'clf__fit_prior': [True,False],\n",
    "    'clf__alpha': [0,0.5,1],\n",
    "From here we refine our selection criteria per tokenized set.\n",
    "\n",
    "**TFIDF=off**\n",
    "\n",
    "Best with no tokenization modification: **57.12%** {'clf__alpha': 1, 'clf__fit_prior': True, 'vect__max_df': 0.2, 'vect__max_features': 150000, 'vect__ngram_range': (1, 2)}\n",
    "\n",
    "Best with stemming: **56.76%**  {clf__alpha: 1,\tclf__fit_prior: True, vect__max_df: 0.2, vect__max_features: 200000, vect__ngram_range: (1, 2)}\n",
    "\n",
    "Best with lemmatization: **56.92%** {'clf__alpha': 1, 'clf__fit_prior': True, 'vect__max_df': 0.3, 'vect__max_features': 150000, 'vect__ngram_range': (1, 2)}\n",
    "\n",
    "**TFIDF=on**\n",
    "\n",
    "Best with no tokenization modification: **58.20%** {'clf__alpha': 0.1, 'clf__fit_prior': True, 'vect__max_df': 0.2, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__norm': 'l2'}\n",
    "\n",
    "Best with stemming: **58.36%** {'clf__alpha': 0.1, 'clf__fit_prior': True, 'vect__max_df': 0.2, 'vect__max_features': 7500, 'vect__ngram_range': (1, 2), 'vect__norm': 'l2'}\n",
    "\n",
    "Best with lemmatization: **58.25%** {'clf__alpha': 0.1, 'clf__fit_prior': True, 'vect__max_df': 0.2, 'vect__max_features': 7500, 'vect__ngram_range': (1, 2), 'vect__norm': 'l2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_df = pd.DataFrame.from_dict(grid_search.cv_results_)\n",
    "# grid_search_df.to_csv(root_path+\"result_tuned_lemmad.csv\", sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We compute the accuracy of the best model on the validation set\n",
    "\n",
    "with balanced dataset  \n",
    "Accuracy on training set: 66.89%  \n",
    "Accuracy on test set: 39.82%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train_lyrics_preprocessed, train_lyrics_stemmed, train_lyrics_lemmad, train_labels\n",
    "# vectorizer = TfidfVectorizer(max_df=0.2, max_features=7500, ngram_range=(1, 2), norm='l2')#add best parameters\n",
    "# classifier = MultinomialNB(alpha=0.1,fit_prior=True)#add best parameters\n",
    "# lyrics_train_vec = vectorizer.fit_transform(train_lyrics_stemmed)\n",
    "# lyrics_test_vec = vectorizer.transform(test_lyrics_stemmed)\n",
    "# classifier.fit(lyrics_train_vec, train_labels)\n",
    "# print(\"Accuracy on training set:\", accuracy_score(train_labels, classifier.predict(lyrics_train_vec)))\n",
    "# print(\"Accuracy on test set:\", accuracy_score(test_labels, classifier.predict(lyrics_test_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Support Vector Machine  <a class=\"anchor\" id=\"SVM\"></a>\n",
    "We test different forms of the vectorised data: stemmed, lemmatized and no token transformation. The step to vectorize the data is quite long so we decide to test different hyperparameters of a model AFTER the vectorization is performed.\n",
    "\n",
    "SVMs are 2 class classifiers. With the LinearSVC there is multiclass support according to a one-vs-the-rest scheme. We also try out other kernels such as XXX\n",
    "\n",
    "#### With no hypterparameter tuning:\n",
    "Accuracy on training set: 0.9945%  \n",
    "Accuracy on validation set: 0.4756%  \n",
    "\n",
    "#### Best model with k-fold cross validation\n",
    "Best accuracy: with TFIDF and lemmatization, for linear kernel.  \n",
    "Accuracy on training set: **93,17%**  \n",
    "Accuracy on validation set: **59,56%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i/10 for i in range(1, 10)]\n",
    "# [i*1000 for i in range(150, 250, 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline combining a text feature extractor with a simple classifier\n",
    "GRID_SEARCH_ON = False\n",
    "pipeline = Pipeline([\n",
    "    # ('vect', CountVectorizer()),\n",
    "    ('vect', TfidfVectorizer()),    \n",
    "    ('clf', LinearSVC()),\n",
    "    # ('clf', SVC()),    \n",
    "])\n",
    "\n",
    "parameters = { \n",
    "    'vect__max_df': [0.5],\n",
    "    # 'vect__max_features': [600000, 700000, 800000,],\n",
    "    'vect__ngram_range': [(1,2)],\n",
    "    'clf__C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    # 'clf__kernel': ['poly'], # 'rbf', 'sigmoid'\n",
    "    # 'clf__gamma': ['scale', 'auto']    \n",
    "}\n",
    "\n",
    "# find the best parameters for both the feature extraction and the classifier\n",
    "if GRID_SEARCH_ON:\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=2, n_jobs=-1, verbose=1)\n",
    "    start = time.time()\n",
    "    grid_search.fit(lyrics_stemmed, train_labels)\n",
    "    end = time.time()\n",
    "    print(\"done in %0.3fs\" % (end - start))\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_df = pd.DataFrame.from_dict(grid_search.cv_results_)\n",
    "# grid_search_df.to_csv(root_path+\"result_SVC_tfidf_stemmed_1.csv\", sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results grid search \n",
    "The different hyperparameters and ranges why are testing:\n",
    "- **tokenizer**: [my_tokenizer, my_tokenizer_stem, my_tokenizer_lemma]. \n",
    "- **max_df**: [0.2, 0.5, 0.8]\n",
    "- **max_features**: [10000, 50000, 100000, 150000, 200000, 250000]\n",
    "- **ngram_range**: unigrams, unigrams and bigrams.\n",
    "- **C**: [0.5, 1, 2].\n",
    "- **TFIDF**: on or off (depends on which vectorizer we use).\n",
    "- **norm**: when TFIDF=on defines the unit norm of each row.\n",
    "\n",
    "Initial parameters run as: parameters = { \n",
    "    'vect__max_df': [0.2, 0.5, 0.8],\n",
    "    'vect__max_features': [10000, 50000, 100000, 200000],\n",
    "    'vect__ngram_range': [(1, 1), (1,2)],\n",
    "    'clf__C': [0.5, 1, 2],\n",
    "From here we refine our selection criteria per tokenized set.\n",
    "\n",
    "**TFIDF=off**  \n",
    "Best with no tokenization modification: **58,56%** {max_df=1, ngram: bigram, C=0.0006}  \n",
    "Best with stemming: **57,4%** {max_df=0.5, max_features=100000, ngram: bigram, C=0.001}  \n",
    "Best with lemmatization: **57,3%** {max_df=0.5, ngram: bigram, C=0.0009}  \n",
    "\n",
    "**TFIDF=on**  \n",
    "Best with no tokenization modification: **58,2%** {max_df=0.4, ngram: bigram, C=0.3}  \n",
    "Best with stemming: **58,4%** {max_df=0.3, ngram: bigram, C=0.4}  \n",
    "Best with lemmatization: **58,5%** {max_df=0.6, ngram: bigram, C=0.305}  \n",
    "\n",
    "With balanced data:  \n",
    "accuracy on training set: 98.53%,\n",
    "on test set: 41.5%  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# vectorizer = TfidfVectorizer(max_df=0.6, ngram_range=(1, 2), norm='l2')#add best parameters\n",
    "# classifier = LinearSVC(C=0.305) #add best parameters\n",
    "# lyrics_train_vec = vectorizer.fit_transform(train_lyrics_lemmad)\n",
    "# lyrics_test_vec = vectorizer.transform(test_lyrics_lemmad)\n",
    "# classifier.fit(lyrics_train_vec, train_labels)\n",
    "# print(\"Accuracy on training set:\", accuracy_score(train_labels, classifier.predict(lyrics_train_vec)))\n",
    "# print(\"Accuracy on test set:\", accuracy_score(test_labels, classifier.predict(lyrics_test_vec)))\n",
    "# print(\"Done in {:03.2f} seconds\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sequenciel model - LSTM  <a class=\"anchor\" id=\"LSTM\"></a>\n",
    "\n",
    "## Parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lyrics_preprocessed, train_lyrics_stemmed, train_lyrics_lemmad, train_labels\n",
    "\n",
    "params = {\n",
    "    'TRAIN_SIZE': len(train_labels),\n",
    "    'VALID_SIZE': len(valid_labels),\n",
    "    'TEST_SIZE': len(test_labels),    \n",
    "\n",
    "    # Hyperparameters\n",
    "    'WORD_VEC_SIZE': 100, # Size of the word embedding vector WARNING: if WORD_VEC_SIZE!=100 the word embedding has to be retrained\n",
    "    'USE_WORD2VEC': 'none', # 'none', 'cbow', 'skip_gram'\n",
    "    'VOCAB_SIZE': 30000, # number of words in vocabulary, -1 if use all the vocab\n",
    "    'MAX_WORDS': 200, # max number of words in song\n",
    "    'PADDING_START': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Word embedding\n",
    "\n",
    "We create word embeddings with Word2Vec, from the gensim package. There are two possible models: continuous bag of words and Skip Gram. The models are trained on the whole data (the 215,824 preprocessed english songs). To avoid re-computing the models, they are saved in the files `model_CBOW_215824_en_songs.model` and `model_Skip_Gram_215824_en_songs.model`.\n",
    "Helpful resources : \n",
    "- https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "- https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
    "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial#Training-the-model\n",
    "\n",
    "**WARNING** : the saved models are computed with the following parameters: `gensim.models.Word2Vec(data, min_count=1, size=WORD_VEC_SIZE, window=5, sg=0, workers=cores-1)` and `gensim.models.Word2Vec(data, min_count=1, size=WORD_VEC_SIZE, window=10, sg=1, workers=cores-1)`. If the WORD_VEC_SIZE **not** equal to 100, the models have to be retrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_models(params):\n",
    "    LOAD_SAVED_W2V_MODELS = params['WORD_VEC_SIZE']==100\n",
    "    if LOAD_SAVED_W2V_MODELS:\n",
    "        model_CBOW = Word2Vec.load(root_path+'model_CBOW_215824_en_songs_.model')\n",
    "        model_Skip_Gram = Word2Vec.load(root_path+'model_Skip_Gram_215824_en_songs_.model')\n",
    "    else:\n",
    "        print(\"Computing word2vec\")\n",
    "        all_lyrics = [my_preprocessor(song, remove_stopwords=False) for song in data_en['lyrics'].to_numpy()]\n",
    "        data = [song.split() for song in all_lyrics]\n",
    "        # Create CBOW model with gensim https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "        model_CBOW = gensim.models.Word2Vec(data, min_count=1, size=params['WORD_VEC_SIZE'], window=5, sg=0, workers=cores-1)\n",
    "        # Create Skip Gram model\n",
    "        model_Skip_Gram = gensim.models.Word2Vec(data, min_count=1, size=params['WORD_VEC_SIZE'], window=10, sg=1, workers=cores-1)\n",
    "        # Save models for later\n",
    "        # model_CBOW.save(root_path+'model_CBOW_215824_en_songs__.model')\n",
    "        # model_Skip_Gram.save(root_path+'model_Skip_Gram_215824_en_songs__.model')\n",
    "        print(\"Done\")\n",
    "    return model_CBOW, model_Skip_Gram\n",
    "\n",
    "model_CBOW, model_Skip_Gram = get_word2vec_models(params)\n",
    "\n",
    "assert len(model_CBOW.wv.vocab)==len(model_Skip_Gram.wv.vocab)\n",
    "print(\"There are\", len(model_CBOW.wv.vocab), \"words in the vocabulary.\")\n",
    "# Checking that the learning makes sense\n",
    "print(\"Cosine similarity between 'love' and 'girl' - CBOW : \", model_CBOW.wv.similarity('love', 'girl'), \"'love' and 'pasta' - CBOW : \", model_CBOW.wv.similarity('love', 'pasta'))\n",
    "print(\"Cosine similarity between 'love' and 'girl' - CBOW : \", model_Skip_Gram.wv.similarity('love', 'girl'), \"'love' and 'pasta' - CBOW : \", model_Skip_Gram.wv.similarity('love', 'pasta'))\n",
    "print(\"{}: {:.4f}\".format(*model_CBOW.wv.most_similar(positive=['woman', 'queen'], negative=['woman'])[0]))\n",
    "print(\"{}: {:.4f}\".format(*model_Skip_Gram.wv.most_similar(positive=['woman', 'queen'], negative=['woman'])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping most common words\n",
    "We want to keep the `VOCAB_SIZE` most common words (for example **30,000** like in the paper). There are **247,779** embedded words in the learnt model, some are probably misspelt.\n",
    "`WORD_TO_INT` associated to each word in the vocabulary a unique value, starting from 1. The value 0 will be reserved for the padding. `MOST_FREQUENT_WORDS` lists the most frequent words in the songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word occurance dictionary\n",
    "\n",
    "def get_most_frequent_words(model, params):\n",
    "    LOAD_WORD_OCC_DICT = True\n",
    "    if LOAD_WORD_OCC_DICT:\n",
    "        word_occurrences = np.load(root_path + 'word_occurrences.npy', allow_pickle='TRUE').item()\n",
    "    else:\n",
    "        all_lyrics_preprocessed = [my_preprocessor(song, remove_stopwords=False) for song in data_en['lyrics'].to_numpy()]\n",
    "        splitted_songs = [song.split() for song in all_lyrics_preprocessed]\n",
    "        word_occurrences = {}\n",
    "        for song in splitted_songs:\n",
    "            for word in song:\n",
    "                if word in word_occurrences.keys():\n",
    "                    word_occurrences[word] += 1\n",
    "                else:\n",
    "                    word_occurrences[word] = 1\n",
    "        np.save(root_path + 'word_occurrences.npy', word_occurrences) \n",
    "    wo_sorted=dict(sorted(word_occurrences.items(), key=lambda x: x[1],reverse=True))\n",
    "    wo_sorted_list = list(wo_sorted.keys())\n",
    "    MOST_FREQUENT_WORDS = wo_sorted_list\n",
    "    if params['VOCAB_SIZE'] != -1:\n",
    "        MOST_FREQUENT_WORDS = wo_sorted_list[:params['VOCAB_SIZE']]\n",
    "    WORD_TO_INT = {word:i+1 for i,word in enumerate(MOST_FREQUENT_WORDS)}\n",
    "    WORD_TO_INT_ALL = {word:i+1 for i,word in enumerate(wo_sorted_list)}\n",
    "    return MOST_FREQUENT_WORDS, WORD_TO_INT, WORD_TO_INT_ALL\n",
    "\n",
    "MOST_FREQUENT_WORDS, WORD_TO_INT, WORD_TO_INT_ALL = get_most_frequent_words(model_CBOW, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song embedding\n",
    "We convert each wong into a vector of integers.\n",
    "\n",
    "If Word2Vec is not used, we simply give a different integer to the words in the vocabulary. The integers range from 1 to `VOCAB_SIZE`, with `VOCAB_SIZE+1` reserved for out of *UNK* and padding = 0.\n",
    "\n",
    "If Word2Vec is used, the `UNK is len(model_CBOW.wv.vocab)` and the `padding is len(model_CBOW.wv.vocab)+1`\n",
    "(words outside of the vocabulary) and `len(model_CBOW.wv.vocab)+1` left for padding.\n",
    "\n",
    "\n",
    "We transform each song so that each word becomes the index of that word in the list `MOST_FREQUENT_WORDS`. The index n°**30001** or VOCAB_SIZE+1 corresponds to the *UNK* words and the index n°**len(model_CBOW.wv.vocab)+1** corresponds to the pad (used later on so that all songs are 600 words long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_songs_idx_balanced(songs, model):\n",
    "    word2index = {token: token_index for token_index, token in enumerate(model.wv.index2word)}\n",
    "    embedded_songs_ = []\n",
    "    for song in songs:\n",
    "        embedded_song = np.array([], dtype = int)\n",
    "        for word in song.split():\n",
    "            if word in WORD_TO_INT:\n",
    "                idx=word2index[word]\n",
    "            else:\n",
    "                idx=len(word2index)\n",
    "            embedded_song = np.append(embedded_song, idx)\n",
    "        embedded_songs_.append(embedded_song)\n",
    "    return np.array(embedded_songs_)\n",
    "params['USE_WORD2VEC']='none'\n",
    "\n",
    "def get_song_idx_embedding(params, train_lyrics_, valid_lyrics_, test_lyrics_):\n",
    "    #'none', 'cbow', 'skip_gram'\n",
    "    if params['USE_WORD2VEC']=='cbow':\n",
    "#         embed_song_idxs_train=embed_songs_idx_balanced(train_lyrics_, model_CBOW)\n",
    "#         embed_song_idxs_valid=embed_songs_idx_balanced(valid_lyrics_, model_CBOW)\n",
    "#         embed_song_idxs_test=embed_songs_idx_balanced(test_lyrics_, model_CBOW) \n",
    "        embed_song_idxs_train = np.load(root_path + 'CBOW_embed_song_idxs_train.npy', allow_pickle=True)\n",
    "        embed_song_idxs_valid = np.load(root_path + 'CBOW_embed_song_idxs_valid.npy', allow_pickle=True)        \n",
    "        embed_song_idxs_test = np.load(root_path + 'CBOW_embed_song_idxs_test.npy', allow_pickle=True)                \n",
    "    elif params['USE_WORD2VEC']=='skip_gram':\n",
    "#         embed_song_idxs_train=embed_songs_idx_balanced(train_lyrics_, model_Skip_Gram)\n",
    "#         embed_song_idxs_valid=embed_songs_idx_balanced(valid_lyrics_, model_Skip_Gram)\n",
    "#         embed_song_idxs_test=embed_songs_idx_balanced(test_lyrics_, model_Skip_Gram) \n",
    "        embed_song_idxs_train = np.load(root_path + 'SG_embed_song_idxs_train.npy', allow_pickle=True)                        \n",
    "        embed_song_idxs_valid = np.load(root_path + 'SG_embed_song_idxs_valid.npy', allow_pickle=True)                                \n",
    "        embed_song_idxs_test = np.load(root_path + 'SG_embed_song_idxs_test.npy', allow_pickle=True)                                        \n",
    "    else:\n",
    "        embed_song_idxs_train = [[WORD_TO_INT.get(w, params['VOCAB_SIZE']+1) for w in song.split()] for song in train_lyrics_]\n",
    "        embed_song_idxs_valid = [[WORD_TO_INT.get(w, params['VOCAB_SIZE']+1) for w in song.split()] for song in valid_lyrics_]   \n",
    "        embed_song_idxs_test = [[WORD_TO_INT.get(w, params['VOCAB_SIZE']+1) for w in song.split()] for song in test_lyrics_]\n",
    "    return embed_song_idxs_train, embed_song_idxs_valid, embed_song_idxs_test\n",
    "\n",
    "embed_song_idxs_train, embed_song_idxs_valid, embed_song_idxs_test = get_song_idx_embedding(params, train_lyrics, valid_lyrics, test_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad data for songs to be same length\n",
    "Some songs are short and longer than other. We cut the long songs and pad the shorter ones. The padding caracter is `len(model_CBOW.wv.vocab)+1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_lengths = [len(x) for x in embed_song_idxs_train]\n",
    "pd.Series(song_lengths).hist()\n",
    "plt.show()\n",
    "pd.Series(song_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_line(song, params):\n",
    "    '''Pads/truncates a song line to have length MAX_WORDS'''\n",
    "    if params['USE_WORD2VEC'] == 'none':\n",
    "        padding_int = 0\n",
    "    else:\n",
    "        padding_int = len(model_CBOW.wv.vocab)+1\n",
    "    size = min(params['MAX_WORDS'], len(song))\n",
    "    to_add = params['MAX_WORDS']-size\n",
    "    # print(params['PADDING_START'])\n",
    "    if params['PADDING_START']:\n",
    "        new_line = np.concatenate((np.full((to_add,), padding_int), song[:size]))\n",
    "    else:\n",
    "        new_line = np.concatenate((song[:size], np.full((to_add,), padding_int)))\n",
    "    return new_line\n",
    "\n",
    "def get_padded_data(params, embed_song_idxs_train_, embed_song_idxs_valid_, embed_song_idxs_test_):\n",
    "    # Get train data\n",
    "    train_data_padded = np.zeros((params['TRAIN_SIZE'], params['MAX_WORDS']), dtype = int)\n",
    "    for i, song in enumerate(embed_song_idxs_train_):\n",
    "        arr = pad_line(song, params)\n",
    "        train_data_padded[i,:] = arr\n",
    "\n",
    "    # Get dev data\n",
    "    valid_data_padded = np.zeros((params['VALID_SIZE'], params['MAX_WORDS']), dtype = int)\n",
    "    for i, song in enumerate(embed_song_idxs_valid_):\n",
    "        arr = pad_line(song, params)\n",
    "        valid_data_padded[i,:] = arr\n",
    "\n",
    "    # Get test data\n",
    "    test_data_padded = np.zeros((params['TEST_SIZE'], params['MAX_WORDS']), dtype = int)\n",
    "    for i, song in enumerate(embed_song_idxs_test_):\n",
    "        arr = pad_line(song, params)\n",
    "        test_data_padded[i,:] = arr\n",
    "    return train_data_padded, valid_data_padded, test_data_padded\n",
    "\n",
    "train_data_padded, valid_data_padded, test_data_padded = get_padded_data(params, embed_song_idxs_train, embed_song_idxs_valid, embed_song_idxs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "\n",
    "Useful ressources:\n",
    "- https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948\n",
    "- https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "- https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79\n",
    "- model.eval(): https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615\n",
    "\n",
    "Other:\n",
    "- https://mlwhiz.com/blog/2018/12/17/text_classification/\n",
    "- https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79\n",
    "- https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948\n",
    "- https://github.com/samarth-agrawal-86/sentiment-analysis-pytorch/blob/master/sentiment_model_class.py\n",
    "- https://github.com/lukysummer/Movie-Review-Sentiment-Analysis-LSTM-Pytorch\n",
    "- https://github.com/lukysummer/Movie-Review-Sentiment-Analysis-LSTM-Pytorch/blob/master/sentiment_analysis_LSTM.py\n",
    "- GlobalMaxPooling: https://stats.stackexchange.com/questions/257321/what-is-global-max-pooling-layer-and-what-is-its-advantage-over-maxpooling-layer\n",
    "- https://www.aclweb.org/anthology/N16-1174.pdf\n",
    "- BERT: https://www.linkedin.com/pulse/fine-tuning-bert-text-classification-20news-group-sharmila-upadhyaya/\n",
    "- Bidi LSTM classification https://www.aclweb.org/anthology/P16-2034/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CBOW.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongsDataset(data.Dataset):\n",
    "    def __init__(self, dataset, labels):\n",
    "        self.dataset = dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = torch.tensor(self.dataset[index], dtype=torch.long)\n",
    "        X = X.to(device)\n",
    "        y = torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        y = y.to(device)        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adjust_learning_rate(optimizer, epoch):\n",
    "#     \"\"\"Sets the learning rate to the initial LR decayed by 10 every X epochs\"\"\"\n",
    "#     lr = args.lr * (0.1 ** (epoch // 10))\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         param_group['lr'] = lr\n",
    "\n",
    "class GenreLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_layers, bidirectional, output_size, batch_size, word_embeddings, drop_p, use_word2vec, song_size, linear_last):\n",
    "        super(GenreLSTM, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "        self.n_layers = n_layers\n",
    "        self.song_size = song_size\n",
    "        self.output_size = output_size\n",
    "        self.linear_last = linear_last\n",
    "        \n",
    "        if use_word2vec!='none':\n",
    "            # self.word_embeddings = nn.Embedding(len(model_CBOW.wv.index2word)+2, n_embed)\n",
    "            self.word_embeddings = nn.Embedding(n_vocab, n_embed)            \n",
    "            self.word_embeddings.weight = nn.Parameter(torch.from_numpy(word_embeddings), requires_grad=True)\n",
    "            self.word_embeddings.require_grad = True\n",
    "        else:\n",
    "            self.word_embeddings = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first=True, bidirectional=bidirectional, dropout=drop_p) \n",
    "        # self.lstm = nn.GRU(n_embed, n_hidden, n_layers, batch_first=True, bidirectional=bidirectional, dropout=drop_p)\n",
    "        # self.lstm2 = nn.GRU(128*2, 64, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d((1, self.n_directions*self.n_hidden))\n",
    "        self.fc = nn.Linear(self.n_directions*n_hidden, output_size)\n",
    "        self.lasthidden = nn.Linear(song_size*output_size, output_size)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        batch_size = sentence.size(0)\n",
    "        # (batch_size, ²)\n",
    "        out = self.word_embeddings(sentence)\n",
    "        # (batch_size, seq_length, n_embed)\n",
    "        out, _ = self.lstm(out)\n",
    "        # (batch_size, seq_length, n_directions*n_hidden)\n",
    "        out = self.dropout(out)\n",
    "        # (batch_size, seq_length, n_directions*n_hidden)\n",
    "        if not self.linear_last:\n",
    "            out = self.maxpool(out)\n",
    "            out = out.squeeze(1)\n",
    "        # (batch_size, n_directions*n_hidden)\n",
    "        # out = out.view(batch_size, -1)\n",
    "        out = self.fc(out)\n",
    "        # out = self.fc(out[:, -1])\n",
    "        # (batch_size, n_output)\n",
    "        if self.linear_last:\n",
    "            out = out.view(batch_size, -1)\n",
    "            out = self.lasthidden(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_LSTM(p, word_embedding):\n",
    "    model = GenreLSTM(p['VOCAB_SIZE']+2, p['WORD_VEC_SIZE'], p['N_HIDDEN'], p['N_LAYERS'], p['BIDIRECTIONAL'], p['N_GENRES'], p['BATCH_SIZE'], word_embedding, p['DROPOUT'], p['USE_WORD2VEC'], p['MAX_WORDS'], p['LINEAR_LAST'])\n",
    "    model = model.to(device)\n",
    "    model.double()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)#,clipnorm=1.25)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    # optimizer = optim.RMSprop(model.parameters(), lr=LR)\n",
    "    # RMSprop, SGD\n",
    "\n",
    "    start = time.time()\n",
    "    train_loss_ = []\n",
    "    test_loss_ = []\n",
    "    train_acc_ = []\n",
    "    test_acc_ = []\n",
    "    ### training proc\n",
    "\n",
    "    for epoch in range(p['N_EPOCHS']):\n",
    "        print(epoch, time.time() - start)\n",
    "\n",
    "        # Training\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        for local_batch, local_labels in train_loader:\n",
    "            # Transfer to GPU\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Model computations\n",
    "            model.zero_grad()\n",
    "\n",
    "            genre_scores = model(local_batch)\n",
    "            loss = loss_function(genre_scores, local_labels)    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calc training acc\n",
    "            _, predicted = torch.max(genre_scores.data, 1)\n",
    "            # print(\"train\", predicted)\n",
    "            # print(\"predicted\", predicted)\n",
    "            # print(\"local_labels\", local_labels)\n",
    "            total_acc += (predicted == local_labels).sum()\n",
    "            total += len(local_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss_.append(total_loss / total)\n",
    "        train_acc_.append(total_acc / total)\n",
    "\n",
    "        # Validation\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        with torch.set_grad_enabled(False):\n",
    "            i = 0\n",
    "            for local_batch, local_labels in valid_loader:\n",
    "                # Transfer to GPU\n",
    "                local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "                genre_scores = model(local_batch)\n",
    "                loss = loss_function(genre_scores, local_labels)\n",
    "                # scheduler.step(loss)\n",
    "                \n",
    "                # calc testing acc\n",
    "                _, predicted = torch.max(genre_scores.data, 1)\n",
    "                # print(\"valid\", predicted)\n",
    "                if i==0:\n",
    "                    pass\n",
    "                    # print(\"predicted\", predicted)\n",
    "                    # print(\"local_labels\", local_labels)\n",
    "                i += 1\n",
    "                total_acc += (predicted == local_labels).sum()\n",
    "                total += len(local_labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            test_loss_.append(total_loss / total)\n",
    "            test_acc_.append(total_acc / total)\n",
    "            print('[Epoch: %3d/%3d] Training Loss: %.6f, Testing Loss: %.6f, Training Acc: %.3f, Testing Acc: %.3f'\n",
    "                  % (epoch, p['N_EPOCHS'], train_loss_[epoch], test_loss_[epoch], train_acc_[epoch], test_acc_[epoch]))\n",
    "    \n",
    "    end = time.time()\n",
    "    return train_loss_, test_loss_, train_acc_, test_acc_, start-end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameter setting\n",
    "params = {\n",
    "    'TRAIN_SIZE': len(train_labels),\n",
    "    'VALID_SIZE': len(valid_labels),\n",
    "    'TEST_SIZE': len(test_labels),    \n",
    "\n",
    "    # Hyperparameters\n",
    "    'N_EPOCHS': 1,\n",
    "    'BATCH_SIZE': 50,\n",
    "    'N_GENRES': 10,\n",
    "    'N_HIDDEN': 32,\n",
    "    'WORD_VEC_SIZE': 100, # Size of the word embedding vector WARNING: if WORD_VEC_SIZE!=100 the word embedding has to be retrained\n",
    "    'LR': 0.01,\n",
    "    'DROPOUT': 0.5,\n",
    "    \n",
    "    # LSTM\n",
    "    'PADDING_START': True,\n",
    "    'MAX_WORDS': 200, # max number of words in song\n",
    "    'VOCAB_SIZE': len(model_CBOW.wv.index2word), # number of words in vocabulary, len(model_CBOW.wv.index2word) if use all the vocab. Does not take into account the padding and UNK\n",
    "    'USE_WORD2VEC': 'none', # 'none', 'cbow', 'skip_gram'\n",
    "    'N_LAYERS': 1, #1 or 2\n",
    "    'BIDIRECTIONAL': False,\n",
    "    'LINEAR_LAST': True\n",
    "}\n",
    "\n",
    "def run_LSTM_with_param(p):\n",
    "    WORD_EMBEDDING=[]\n",
    "    model_CBOW, model_Skip_Gram = get_word2vec_models(p)\n",
    "    if p['USE_WORD2VEC'] != 'none':\n",
    "        if p['USE_WORD2VEC'] == 'cbow':\n",
    "            model=model_CBOW\n",
    "        else:\n",
    "            model=model_Skip_Gram\n",
    "        WORD_EMBEDDING=np.concatenate((model.wv[model.wv.index2word], [np.random.rand((p['WORD_VEC_SIZE']))], [np.random.rand((p['WORD_VEC_SIZE']))]))\n",
    "        WORD_EMBEDDING.astype(np.double)\n",
    "    len(WORD_EMBEDDING)\n",
    "\n",
    "    MOST_FREQUENT_WORDS, WORD_TO_INT, WORD_TO_INT_ALL = get_most_frequent_words(model_CBOW, p)\n",
    "    embed_song_idxs_train, embed_song_idxs_valid, embed_song_idxs_test = get_song_idx_embedding(p, train_lyrics, valid_lyrics, test_lyrics)\n",
    "    train_data_padded, valid_data_padded, test_data_padded = get_padded_data(p, embed_song_idxs_train, embed_song_idxs_valid, embed_song_idxs_test)\n",
    "\n",
    "    train_dataset = SongsDataset(train_data_padded, train_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=p['BATCH_SIZE'], shuffle=True)\n",
    "    valid_dataset = SongsDataset(valid_data_padded, valid_labels)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=p['BATCH_SIZE'], shuffle=True)\n",
    "    test_dataset = SongsDataset(test_data_padded, test_labels)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=p['BATCH_SIZE'], shuffle=True)\n",
    "    return run_LSTM(p, WORD_EMBEDDING)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'TRAIN_SIZE': len(train_labels),\n",
    "    'VALID_SIZE': len(valid_labels),\n",
    "    'TEST_SIZE': len(test_labels),    \n",
    "\n",
    "    # Hyperparameters\n",
    "    'N_EPOCHS': 1,\n",
    "    'BATCH_SIZE': 50,\n",
    "    'N_GENRES': 10,\n",
    "    'N_HIDDEN': 32,\n",
    "    'WORD_VEC_SIZE': 100, # Size of the word embedding vector WARNING: if WORD_VEC_SIZE!=100 the word embedding has to be retrained\n",
    "    'LR': 0.01,\n",
    "    'DROPOUT': 0.5,\n",
    "    \n",
    "    # LSTM\n",
    "    'PADDING_START': True,\n",
    "    'MAX_WORDS': 200, # max number of words in song\n",
    "    'VOCAB_SIZE': 30000, # number of words in vocabulary, -1 if use all the vocab. Does not take into account the padding and UNK\n",
    "    'USE_WORD2VEC': 'none', # 'none', 'cbow', 'skip_gram'\n",
    "    'N_LAYERS': 1, #1 or 2\n",
    "    'BIDIRECTIONAL': False,\n",
    "    'LINEAR_LAST': True\n",
    "}\n",
    "train_loss_, test_loss_, train_acc_, test_acc_, time_coucou = run_LSTM_with_param({'TRAIN_SIZE': len(train_labels), 'VALID_SIZE': len(valid_labels), 'TEST_SIZE': len(test_labels), 'N_EPOCHS': 1, 'BATCH_SIZE': 50, 'N_GENRES': 10, 'N_HIDDEN': 32, 'WORD_VEC_SIZE': 100, 'LR': 0.01, 'DROPOUT': 0.5, 'PADDING_START': False, 'MAX_WORDS': 200, 'VOCAB_SIZE': 30000, 'USE_WORD2VEC': 'cbow', 'N_LAYERS': 1, 'BIDIRECTIONAL': False, 'LINEAR_LAST': False})\n",
    "print(train_loss_, test_loss_, train_acc_, test_acc_, time_coucou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {'TRAIN_SIZE': len(train_labels), 'VALID_SIZE': len(valid_labels), 'TEST_SIZE': len(test_labels), 'N_EPOCHS': 1, 'BATCH_SIZE': 50, 'N_GENRES': 10, 'N_HIDDEN': 32, 'WORD_VEC_SIZE': 100, 'LR': 0.01, 'DROPOUT': 0.5, 'PADDING_START': False, 'MAX_WORDS': 200, 'VOCAB_SIZE': 30000, 'USE_WORD2VEC': 'cbow', 'N_LAYERS': 1, 'BIDIRECTIONAL': False, 'LINEAR_LAST': False},\n",
    "    {'TRAIN_SIZE': len(train_labels), 'VALID_SIZE': len(valid_labels), 'TEST_SIZE': len(test_labels), 'N_EPOCHS': 1, 'BATCH_SIZE': 50, 'N_GENRES': 10, 'N_HIDDEN': 32, 'WORD_VEC_SIZE': 100, 'LR': 0.01, 'DROPOUT': 0.5, 'PADDING_START': True, 'MAX_WORDS': 200, 'VOCAB_SIZE': 30000, 'USE_WORD2VEC': 'cbow', 'N_LAYERS': 1, 'BIDIRECTIONAL': False, 'LINEAR_LAST': False},    \n",
    "    {'TRAIN_SIZE': len(train_labels), 'VALID_SIZE': len(valid_labels), 'TEST_SIZE': len(test_labels), 'N_EPOCHS': 1, 'BATCH_SIZE': 50, 'N_GENRES': 10, 'N_HIDDEN': 32, 'WORD_VEC_SIZE': 100, 'LR': 0.01, 'DROPOUT': 0.5, 'PADDING_START': False, 'MAX_WORDS': 50, 'VOCAB_SIZE': 30000, 'USE_WORD2VEC': 'cbow', 'N_LAYERS': 1, 'BIDIRECTIONAL': False, 'LINEAR_LAST': False},    \n",
    "    {'TRAIN_SIZE': len(train_labels), 'VALID_SIZE': len(valid_labels), 'TEST_SIZE': len(test_labels), 'N_EPOCHS': 1, 'BATCH_SIZE': 50, 'N_GENRES': 10, 'N_HIDDEN': 32, 'WORD_VEC_SIZE': 100, 'LR': 0.01, 'DROPOUT': 0.5, 'PADDING_START': False, 'MAX_WORDS': 600, 'VOCAB_SIZE': 30000, 'USE_WORD2VEC': 'cbow', 'N_LAYERS': 1, 'BIDIRECTIONAL': False, 'LINEAR_LAST': False},        \n",
    "    {'TRAIN_SIZE': len(train_labels), 'VALID_SIZE': len(valid_labels), 'TEST_SIZE': len(test_labels), 'N_EPOCHS': 1, 'BATCH_SIZE': 50, 'N_GENRES': 10, 'N_HIDDEN': 32, 'WORD_VEC_SIZE': 100, 'LR': 0.01, 'DROPOUT': 0.5, 'PADDING_START': False, 'MAX_WORDS': 200, 'VOCAB_SIZE': len(model_CBOW.wv.index2word), 'USE_WORD2VEC': 'cbow', 'N_LAYERS': 1, 'BIDIRECTIONAL': False, 'LINEAR_LAST': False},    \n",
    "    {'TRAIN_SIZE': len(train_labels), 'VALID_SIZE': len(valid_labels), 'TEST_SIZE': len(test_labels), 'N_EPOCHS': 1, 'BATCH_SIZE': 50, 'N_GENRES': 10, 'N_HIDDEN': 32, 'WORD_VEC_SIZE': 100, 'LR': 0.01, 'DROPOUT': 0.5, 'PADDING_START': False, 'MAX_WORDS': 200, 'VOCAB_SIZE': 30000, 'USE_WORD2VEC': 'none', 'N_LAYERS': 1, 'BIDIRECTIONAL': False, 'LINEAR_LAST': False},    \n",
    "    {'TRAIN_SIZE': len(train_labels), 'VALID_SIZE': len(valid_labels), 'TEST_SIZE': len(test_labels), 'N_EPOCHS': 1, 'BATCH_SIZE': 50, 'N_GENRES': 10, 'N_HIDDEN': 32, 'WORD_VEC_SIZE': 100, 'LR': 0.01, 'DROPOUT': 0.5, 'PADDING_START': False, 'MAX_WORDS': 200, 'VOCAB_SIZE': 30000, 'USE_WORD2VEC': 'skip_gram', 'N_LAYERS': 1, 'BIDIRECTIONAL': False, 'LINEAR_LAST': False},        \n",
    "    {'TRAIN_SIZE': len(train_labels), 'VALID_SIZE': len(valid_labels), 'TEST_SIZE': len(test_labels), 'N_EPOCHS': 1, 'BATCH_SIZE': 50, 'N_GENRES': 10, 'N_HIDDEN': 32, 'WORD_VEC_SIZE': 100, 'LR': 0.01, 'DROPOUT': 0.5, 'PADDING_START': False, 'MAX_WORDS': 200, 'VOCAB_SIZE': 30000, 'USE_WORD2VEC': 'cbow', 'N_LAYERS': 2, 'BIDIRECTIONAL': False, 'LINEAR_LAST': False},\n",
    "    {'TRAIN_SIZE': len(train_labels), 'VALID_SIZE': len(valid_labels), 'TEST_SIZE': len(test_labels), 'N_EPOCHS': 1, 'BATCH_SIZE': 50, 'N_GENRES': 10, 'N_HIDDEN': 32, 'WORD_VEC_SIZE': 100, 'LR': 0.01, 'DROPOUT': 0.5, 'PADDING_START': False, 'MAX_WORDS': 200, 'VOCAB_SIZE': 30000, 'USE_WORD2VEC': 'cbow', 'N_LAYERS': 1, 'BIDIRECTIONAL': True, 'LINEAR_LAST': False},\n",
    "    {'TRAIN_SIZE': len(train_labels), 'VALID_SIZE': len(valid_labels), 'TEST_SIZE': len(test_labels), 'N_EPOCHS': 1, 'BATCH_SIZE': 50, 'N_GENRES': 10, 'N_HIDDEN': 32, 'WORD_VEC_SIZE': 100, 'LR': 0.01, 'DROPOUT': 0.5, 'PADDING_START': False, 'MAX_WORDS': 200, 'VOCAB_SIZE': 30000, 'USE_WORD2VEC': 'cbow', 'N_LAYERS': 1, 'BIDIRECTIONAL': False, 'LINEAR_LAST': True},\n",
    "    {'TRAIN_SIZE': len(train_labels), 'VALID_SIZE': len(valid_labels), 'TEST_SIZE': len(test_labels), 'N_EPOCHS': 1, 'BATCH_SIZE': 50, 'N_GENRES': 10, 'N_HIDDEN': 32, 'WORD_VEC_SIZE': 100, 'LR': 0.01, 'DROPOUT': 0.5, 'PADDING_START': False, 'MAX_WORDS': 200, 'VOCAB_SIZE': len(model_CBOW.wv.index2word), 'USE_WORD2VEC': 'cbow', 'N_LAYERS': 2, 'BIDIRECTIONAL': True, 'LINEAR_LAST': True},\n",
    "]\n",
    "\n",
    "# params = [{'TRAIN_SIZE': len(train_labels), 'VALID_SIZE': len(valid_labels), 'TEST_SIZE': len(test_labels), 'N_EPOCHS': 1, 'BATCH_SIZE': 50, 'N_GENRES': 10, 'N_HIDDEN': 32, 'WORD_VEC_SIZE': 100, 'LR': 0.01, 'DROPOUT': 0.5, 'PADDING_START': False, 'MAX_WORDS': 200, 'VOCAB_SIZE': 30000, 'USE_WORD2VEC': 'cbow', 'N_LAYERS': 1, 'BIDIRECTIONAL': False, 'LINEAR_LAST': False}]\n",
    "\n",
    "dict_ = {}\n",
    "for i in range(len(params)):\n",
    "    # print(params[i])\n",
    "    # train_loss_, test_loss_, train_acc_, test_acc_, time_run = run_LSTM_with_param(params[i])\n",
    "    # dict_[i] = {'train_loss_':train_loss_, 'test_loss_':test_loss_, 'train_acc_':train_acc_, 'test_acc_':test_acc_, 'time_run': time_run}\n",
    "    try:\n",
    "        print('running', i)\n",
    "        train_loss_, test_loss_, train_acc_, test_acc_, time_run = run_LSTM_with_param(params[i])\n",
    "        dict_[i] = {'train_loss_':train_loss_, 'test_loss_':test_loss_, 'train_acc_':train_acc_, 'test_acc_':test_acc_, 'time': time}\n",
    "    except:\n",
    "        print(\"not working\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.layers import Embedding, LSTM, Dropout, GlobalMaxPooling1D\n",
    "# from keras.layers import Dense, Bidirectional, GRU\n",
    "# from keras import optimizers\n",
    "\n",
    "# ## DEFINE PARAMETERS\n",
    "\n",
    "# # MAX_WORDS = 600 # max number of words in song\n",
    "# # vocab_size = 30000 # and 1 for unknown, and 1 for mask\n",
    "\n",
    "# MAX_WORDS = 150\n",
    "# vocab_size = VOCAB_SIZE + 2\n",
    "# if USE_WORD2VEC:\n",
    "#       vocab_size -= 1\n",
    "# learning_rate = .01\n",
    "# training_epochs = 20\n",
    "# batch_size = 64\n",
    "# embed_size = 100\n",
    "# dropout = 0.5\n",
    "# n_hidden = 50 # number of hidden states in LSTM\n",
    "# print(np.shape(train_data))\n",
    "# print('MAX_WORDS', MAX_WORDS, '; vocab_size', vocab_size, '; learning_rate', learning_rate, '; training_epochs', training_epochs, '; batch_size', batch_size, '; embed_size', embed_size, '; dropout', dropout, '; n_hidden', n_hidden)\n",
    "\n",
    "# num_genres = 10\n",
    "# num_classes = num_genres\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, embed_size, weights=[WORD_EMBEDDING], input_length=MAX_WORDS, trainable=True)) # \n",
    "# model.add(LSTM(n_hidden, activation='sigmoid', return_sequences=True))\n",
    "# # model.add(Bidirectional(GRU(n_hidden, return_sequences=True, init='he_normal', inner_init='he_normal', inner_activation='sigmoid'), name='bidirect_word'))\n",
    "# model.add(Dropout(dropout))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(num_genres, activation='softmax'))\n",
    "\n",
    "# optimizer = optimizers.RMSprop(lr=learning_rate)\n",
    "# model.compile(loss='sparse_categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['acc'])\n",
    "\n",
    "# print(\"model fitting - Baseline LSTM\")\n",
    "# print(model.summary())\n",
    "# earlystopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "# # checkpointer = ModelCheckpoint(filepath='results/lstm/lstmbest.hdf5',verbose=1,save_best_only=True)\n",
    "# hist = model.fit(train_data, train_labels, validation_data=(valid_data, valid_labels),\n",
    "#           nb_epoch=training_epochs, batch_size=batch_size, callbacks=[earlystopping])\n",
    "# print(hist.history)\n",
    "# # model.save('results/lstm/lstm.h5')\n",
    "\n",
    "# # evals = model.evaluate(test_data, test_labels)\n",
    "# # print(\"Test accuracy:\", evals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Installing keras-bert and keras adapter\n",
    "# !pip install -q keras-bert keras-rectified-adam\n",
    "# !wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "# !unzip -o uncased_L-12_H-768_A-12.zip\n",
    "\n",
    "# # Parameters\n",
    "# SEQ_LEN = 128\n",
    "# BATCH_SIZE = 128\n",
    "# EPOCHS = 30\n",
    "# LR = 1e-4\n",
    "\n",
    "# # Pretrained model path\n",
    "# import os\n",
    "\n",
    "# pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "# config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "# checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "# vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "\n",
    "# # TF_KERAS must be added to environment variables in order to use TPU\n",
    "# os.environ['TF_KERAS'] = '1'\n",
    "\n",
    "# # Initialize TPU strategy\n",
    "# import tensorflow as tf\n",
    "# from keras_bert import get_custom_objects\n",
    "# TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "# resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
    "# tf.contrib.distribute.initialize_tpu_system(resolver)\n",
    "# strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
    "\n",
    "# # load bert model\n",
    "# import codecs\n",
    "# from keras_bert import load_trained_model_from_checkpoint\n",
    "# token_dict = {}\n",
    "# with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "#     for line in reader:\n",
    "#         token = line.strip()\n",
    "#         token_dict[token] = len(token_dict)\n",
    "# with strategy.scope():\n",
    "#     model = load_trained_model_from_checkpoint(\n",
    "#         config_path,\n",
    "#         checkpoint_path,\n",
    "#         training=True,\n",
    "#         trainable=True,\n",
    "#         seq_len=SEQ_LEN,\n",
    "#     )\n",
    "# model.summary()\n",
    "\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from keras_bert import Tokenizer\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from google.colab import files\n",
    "# import json\n",
    "\n",
    "# train_data = np.load('train_data_colab_bert.npy', allow_pickle=True)\n",
    "# train_labels = np.load('train_labels_colab_bert.npy', allow_pickle=True)\n",
    "# valid_data = np.load('valid_data_colab_bert.npy', allow_pickle=True)\n",
    "# valid_labels = np.load('valid_labels_colab_bert.npy', allow_pickle=True)\n",
    "# train_x = [train_data, np.zeros_like(train_data)]\n",
    "# test_x = [valid_data, np.zeros_like(valid_data)]\n",
    "# train_y = train_labels\n",
    "# test_y = valid_labels\n",
    "\n",
    "# # Build Custom Model\n",
    "# from tensorflow.python import keras\n",
    "# from keras_radam import RAdam\n",
    "\n",
    "# with strategy.scope():\n",
    "#     inputs = model.inputs[:2]\n",
    "#     dense = model.get_layer('NSP-Dense').output\n",
    "#     outputs = keras.layers.Dense(units=20, activation='softmax')(dense)\n",
    "#     model = keras.models.Model(inputs, outputs)\n",
    "#     model.compile(\n",
    "#         RAdam(lr=LR),\n",
    "#         loss='sparse_categorical_crossentropy',\n",
    "#         metrics=['sparse_categorical_accuracy'],\n",
    "#     )\n",
    "\n",
    "# #  Initialize Variables\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.keras.backend as K\n",
    "# sess = K.get_session()\n",
    "# uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n",
    "# init_op = tf.variables_initializer(\n",
    "#     [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\n",
    "# )\n",
    "# sess.run(init_op)\n",
    "\n",
    "# # Fit\n",
    "# model.fit(\n",
    "#     train_x,\n",
    "#     train_y,\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     validation_data=(test_x, test_y)\n",
    "# )\n",
    "\n",
    "# # @title Predict\n",
    "# predicts = model.predict(test_x, verbose=True).argmax(axis=-1)\n",
    "\n",
    "# # @title Accuracy\n",
    "# print(np.sum(test_y == predicts) / test_y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.load('train_data_colab_bert.npy', allow_pickle=True)\n",
    "# train_labels = np.load('train_labels_colab_bert.npy', allow_pickle=True)\n",
    "# valid_data = np.load('valid_data_colab_bert.npy', allow_pickle=True)\n",
    "# valid_labels = np.load('valid_labels_colab_bert.npy', allow_pickle=True)\n",
    "# train_x = [train_data, np.zeros_like(train_data)]\n",
    "# test_x = [valid_data, np.zeros_like(valid_data)]\n",
    "# train_y = train_labels\n",
    "# test_y = valid_labels\n",
    "\n",
    "# from google.colab import files\n",
    "# import json\n",
    "\n",
    "# f=open(\"LSTM_BERT.txt\",\"w+\")\n",
    "\n",
    "# f.write(json.dumps(hist.history))\n",
    "# f.write(\"predicts\")\n",
    "# predicts = model.predict(test_x, verbose=True).argmax(axis=-1)\n",
    "# f.write(str(predicts))\n",
    "# f.write(\"accuracy\")\n",
    "# f.write(str(np.sum(test_y == predicts) / test_y.shape[0]))\n",
    "# f.close()\n",
    "\n",
    "# time.sleep(300)\n",
    "# files.download('LSTM_BERT.txt')"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
