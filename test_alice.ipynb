{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP550 - Final Project\n",
    "\n",
    "Links:\n",
    "- https://www.kaggle.com/ficklemaverick/lyrics-generator\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "- https://www.kaggle.com/danofer/music-lyrics-clean-export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from langdetect import detect\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data_path = \"data/lyrics.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.read_csv(\"data/lyrics.csv\")\n",
    "print(len(data_all), \"songs in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the lines that are not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_data = data_all[pd.notnull(data_all['lyrics'])]\n",
    "lyrics_data_light = lyrics_data[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing steps\n",
    "The lyrics need to be cleaned before we can use them.\n",
    "- remove \\n line breaks\n",
    "- remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lyrics_data_light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = data['lyrics'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = nltk.word_tokenize(sentence)\n",
    "# print(len(a))\n",
    "en_songs = []\n",
    "for song in lyrics:\n",
    "    try:\n",
    "        lang = detect(song)\n",
    "        if lang == 'en':\n",
    "            en_songs.append(song)\n",
    "    except:\n",
    "        pass\n",
    "# lyrics_en = [song for song in lyrics if detect(song)=='en']\n",
    "# print(\"There are\", len(lyrics), \"songs and\", len(lyrics_en), \"English songs\")\n",
    "# # detect(\"War doesn't show who's right, just who's left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", len(en_songs), \"English songs in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NLTKTokenizer(object):\n",
    "#     \"\"\"\n",
    "#     This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.tknzr = TweetTokenizer(preserve_case=False, strip_handles=False, reduce_len=False)\n",
    "\n",
    "#     def tokenize(self, text):\n",
    "#         tokens = self.tknzr.tokenize(text)\n",
    "#         return tokens\n",
    "\n",
    "\n",
    "# class Stemmer(object):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "#     def stem(self, token):\n",
    "#         \"\"\"\n",
    "#         token: a string that contain a token\n",
    "#         \"\"\"\n",
    "#         stemed_token = self.stemmer.stem(token)\n",
    "#         return stemed_token\n",
    "\n",
    "# class PosFilter(object):\n",
    "#     def __init__(self):\n",
    "#         return\n",
    "\n",
    "    \n",
    "\n",
    "# class TwitterPreprocessing(object):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         # Il faut convertir le POS retourné par nltk.pos_tag en un label qui peut être utilisé par le WordNetLemmatizer.\n",
    "#         # Voici la correspondance entre le nltk_POS et le POS utilisé par le WordNetLemmatizer\n",
    "#         # ['NN', 'NNS', 'NNP', 'NNPS'] (commence par un N) -> wordnet.NOUN\n",
    "#         # ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] (commence par un V) -> wordnet.VERB\n",
    "#         # ['RB', 'RBR', 'RBS'] (commence par un R) -> wordnet.ADV\n",
    "#         # ['JJ', 'JJR', 'JJS'] (commence par un J) -> wordnet.ADJ\n",
    "#         # Pour tout autre POS on utilise wordnet.NOUN\n",
    "#         self.POS_correspondance = {'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV, 'J': wordnet.ADJ}\n",
    "#         self.html_escape_table = {\"amp\": \"&\", \"quot\": '\"', \"apos\": \"'\", \"gt\": \">\", \"lt\": \"<\", }\n",
    "#         self.stop_words = set(stopwords.words('english'))\n",
    "#         self.stop_words.update(['retweet', 'u', 'im','rt'])\n",
    "#         self.stop_words.update(['retweet', 'u', 'im', 'say', 'give,', 'look', 'rt', 'new', 'year', 'make', 'go', 'take', 'one', 'day', 'late', 'via', 'today', 'time', 'first'])        \n",
    "#         self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#     def noise_removal(self, tweet):\n",
    "#         tweet = [self.html_escape_table.get(w, w) for w in tweet]\n",
    "#         tweet = [re.sub(\"[\\.|,|&|\\\"|\\'|/]\", \"\", w) for w in tweet]\n",
    "#         tweet = [w for w in tweet if not w.startswith(u'\\x8f')]\n",
    "#         tweet = [w for w in tweet if not w.startswith(u'\\x9d')]        \n",
    "#         tweet = [w for w in tweet if \"http\" not in w]\n",
    "#         tweet = [w for w in tweet if \"www\" not in w]        \n",
    "#         tweet = [w for w in tweet if len(w)>1]\n",
    "#         return tweet\n",
    "\n",
    "#     def filter_pos(self, tweet):\n",
    "#         return terms_of_tweet(tweet)\n",
    "    \n",
    "#     def lemmatize(self, tweet):\n",
    "#         tweet_with_POS = nltk.pos_tag(tweet)\n",
    "#         lemmatized_tweet = [self.lemmatizer.lemmatize(w[0], self.POS_correspondance.get(w[1][0], wordnet.NOUN)) for w in tweet_with_POS]\n",
    "#         return lemmatized_tweet\n",
    "    \n",
    "#     def remove_stop_words(self, tweet):\n",
    "#         tweet = [t for t in tweet if t not in self.stop_words]\n",
    "#         return tweet\n",
    "\n",
    "#     def preprocess(self, tweet, pos_filter):\n",
    "#         \"\"\"\n",
    "#         tweet: original tweet\n",
    "#         \"\"\"\n",
    "#         # Etape 1\n",
    "#         tweet = self.noise_removal(tweet)\n",
    "        \n",
    "#         # Etape 1.b\n",
    "#         if(pos_filter):\n",
    "#             tweet = self.filter_pos(tweet)\n",
    "#         # Etape 2\n",
    "#         tweet = self.lemmatize(tweet)\n",
    "#         # Etape 3\n",
    "#         tweet = self.remove_stop_words(tweet)\n",
    "#         return tweet\n",
    "\n",
    "    \n",
    "# class PreprocessingPipeline:\n",
    "\n",
    "#     def __init__(self, tokenization, twitterPreprocessing, stemming, pos_filter=False):\n",
    "#         \"\"\"\n",
    "#         tokenization: enable or disable tokenization.\n",
    "#         twitterPreprocessing: enable or disable twitter preprocessing.\n",
    "#         stemming: enable or disable stemming.\n",
    "#         \"\"\"\n",
    "\n",
    "#         self.tokenizer = NLTKTokenizer() if tokenization else SpaceTokenizer()\n",
    "#         self.twitterPreprocesser = TwitterPreprocessing() if twitterPreprocessing else None\n",
    "#         self.stemmer = Stemmer() if stemming else None\n",
    "#         self.pos_filter = pos_filter\n",
    "#         self.PosFilter = PosFilter() if pos_filter else None\n",
    "\n",
    "#     def preprocess(self, tweet):\n",
    "#         \"\"\"\n",
    "#         Transform the raw data\n",
    "\n",
    "#         tokenization: boolean value.\n",
    "#         twitterPreprocessing: boolean value. Apply the\n",
    "#         stemming: boolean value.\n",
    "#         \"\"\"\n",
    "\n",
    "#         tokens = self.tokenizer.tokenize(tweet)\n",
    "        \n",
    "#         ## Twitter preprocessing before stem\n",
    "#         if self.twitterPreprocesser:\n",
    "#             tokens = self.twitterPreprocesser.preprocess(tokens,self.pos_filter)\n",
    "        \n",
    "#         if self.stemmer:\n",
    "#             tokens = list(map(self.stemmer.stem, tokens))\n",
    "#         return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english', max_features=5000)\n",
    "X = vectorizer.fit_transform(lyrics_en)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
